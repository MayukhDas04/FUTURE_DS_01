# -*- coding: utf-8 -*-
"""TASK1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LSOr7xLIxHhUmRuIditELJ3WYtDST5c3

**Reasoning**:
Load the sentiment dataset into a pandas DataFrame and display the first few rows.
"""

import pandas as pd

try:
    df = pd.read_csv('sentimentdataset (2).csv', encoding='latin-1')
    display(df.head())
except FileNotFoundError:
    print("Error: 'sentimentdataset (2).csv' not found.")
except pd.errors.ParserError:
    print("Error: Could not parse the CSV file. Check the file format and encoding.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")

"""## Data exploration

### Subtask:
Explore the loaded dataset to understand its structure, identify relevant columns, check for missing values, and determine the distribution of sentiment.

**Reasoning**:
Examine the shape of the DataFrame, display data types, check for missing values, analyze the distribution of the 'Sentiment' column, investigate the 'Text' column, and create a summary report with a visualization.
"""

import matplotlib.pyplot as plt

# 1. Examine the shape
print(f"DataFrame shape: {df.shape}")

# 2. Display data types
print("\nData Types:")
print(df.dtypes)

# 3. Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Check for empty strings in 'Text' and 'Sentiment'
print("\nEmpty strings in 'Text':", (df['Text'] == '').sum())
print("\nEmpty strings in 'Sentiment':", (df['Sentiment'] == '').sum())


# 4. Analyze sentiment distribution
sentiment_counts = df['Sentiment'].value_counts()
print("\nSentiment Distribution:")
print(sentiment_counts)

plt.figure(figsize=(8, 6))
sentiment_counts.plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])
plt.title('Distribution of Sentiment Labels')
plt.xlabel('Sentiment')
plt.ylabel('Frequency')
plt.show()


# 5. Investigate the 'Text' column
# Calculate text lengths (character count)
df['Text_Length'] = df['Text'].str.len()

# Basic statistics for text length
print("\nText Length Statistics:")
print(df['Text_Length'].describe())

# Example: Check for special characters (you can customize this)
# df['Special_Chars'] = df['Text'].str.contains(r'[^\w\s]', regex=True)
# print("\nSpecial Characters:")
# print(df['Special_Chars'].value_counts())


# 6. Summarize findings (report)
print("\nSummary Report:")
print(f"Number of rows: {df.shape[0]}, Number of columns: {df.shape[1]}")
print("\nKey Column Data Types:")
print(df[['Sentiment', 'Text', 'Timestamp']].dtypes)
print("\nMissing Value Percentages:")
print( (df.isnull().sum() / len(df)) * 100)

print("\nObservations:")
print("The dataset appears to contain information about social media posts with associated sentiment.  There are potentially redundant columns ('Unnamed: 0.1', 'Unnamed: 0').  The 'Sentiment' distribution shows some imbalance.  Further investigation of missing values and text cleaning is needed. The 'Timestamp' column needs to be converted to datetime.")

"""## Data cleaning

### Subtask:
Clean the text data by removing irrelevant characters, handling missing values (if any), and converting text to lowercase.

**Reasoning**:
Remove redundant columns, convert the 'Timestamp' column to datetime objects, remove rows with missing or empty 'Text', convert text to lowercase, remove irrelevant characters, and handle missing 'Sentiment' values.
"""

import pandas as pd

# Drop redundant columns
df = df.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1, errors='ignore')

# Convert 'Timestamp' to datetime, coercing errors
df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')

# Remove rows with missing or empty 'Text'
df = df.dropna(subset=['Text']).reset_index(drop=True)
df = df[df['Text'] != '']

# Convert text to lowercase
df['Text'] = df['Text'].str.lower()

# Remove irrelevant characters using regular expressions (preserving emojis)
import re
def clean_text(text):
    text = re.sub(r'[^\w\s#@]', '', text) # Remove punctuation except @ and #
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    return text
df['Text'] = df['Text'].apply(clean_text)

# Handle missing 'Sentiment' (remove rows with missing sentiment)
df = df.dropna(subset=['Sentiment']).reset_index(drop=True)

display(df.head())

"""## Data wrangling

### Subtask:
Prepare the text data for NLP analysis.

**Reasoning**:
The previous code failed due to missing NLTK resources. Download the necessary resource `wordnet` before retrying the code.
"""

import nltk
nltk.download('wordnet')

"""**Reasoning**:
Retry the preprocessing code after downloading the required resource.


"""

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer
import re

# Customize stop words (example: remove 'not' to preserve negation)
stop_words = stopwords.words('english')
stop_words.remove('not')

# Tokenizer that handles emojis
tokenizer = TweetTokenizer()

def preprocess_text(text):
    # Tokenize the text
    tokens = tokenizer.tokenize(text)

    # Remove stop words and punctuation
    tokens = [token for token in tokens if token.lower() not in stop_words and re.match(r'\b[a-zA-Z]+\b', token)]

    # Lemmatize
    lemmatized_tokens = [wnl.lemmatize(token) for token in tokens]

    return lemmatized_tokens

# Apply preprocessing
wnl = WordNetLemmatizer()
df['Processed_Tokens'] = df['Text'].apply(preprocess_text)

display(df.head())

"""## Data analysis

### Subtask:
Perform sentiment analysis and topic grouping.

**Reasoning**:
Perform sentiment analysis using TextBlob, categorize sentiments, and visualize the results.
"""

from textblob import TextBlob
import matplotlib.pyplot as plt

# 1. Sentiment Analysis
df['Polarity'] = df['Text'].apply(lambda text: TextBlob(text).sentiment.polarity)
df['Subjectivity'] = df['Text'].apply(lambda text: TextBlob(text).sentiment.subjectivity)

# 2. Sentiment Categorization
def categorize_sentiment(polarity):
    if polarity > 0.1:
        return 'Positive'
    elif polarity < -0.1:
        return 'Negative'
    else:
        return 'Neutral'

df['Sentiment_Category'] = df['Polarity'].apply(categorize_sentiment)

# 3. Group by Sentiment Category (and potentially topic if available)
sentiment_counts = df['Sentiment_Category'].value_counts()
print(sentiment_counts)

plt.figure(figsize=(8, 6))
sentiment_counts.plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])
plt.title('Distribution of Sentiment Categories')
plt.xlabel('Sentiment Category')
plt.ylabel('Frequency')
plt.show()

#If topic information is available (e.g., Hashtags)
if 'Hashtags' in df.columns:
    # Split hashtags into individual hashtags and explode the column
    df['Hashtags'] = df['Hashtags'].str.split()
    hashtags_df = df.explode('Hashtags')

    # Calculate the average sentiment for each hashtag
    average_sentiment_by_hashtag = hashtags_df.groupby('Hashtags')['Polarity'].mean().sort_values(ascending=False)

    # Display the results
    print("\nAverage Sentiment by Hashtag:")
    print(average_sentiment_by_hashtag)

    # Visualize the average sentiment by topic
    plt.figure(figsize=(12, 6))
    average_sentiment_by_hashtag.plot(kind='bar', color='skyblue')
    plt.title('Average Sentiment Score by Hashtag')
    plt.xlabel('Hashtag')
    plt.ylabel('Average Polarity Score')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

import pandas as pd

try:
    df = pd.read_csv('sentimentdataset (2).csv', encoding='latin-1')
except FileNotFoundError:
    print("Error: 'sentimentdataset (2).csv' not found.")
    # Handle the error appropriately, e.g., exit or use a default dataframe
except pd.errors.ParserError:
    print("Error: Could not parse the CSV file. Check the file format and encoding.")
    # Handle the error appropriately
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    # Handle the error appropriately

"""**Reasoning**:
I need to execute the visualization code again, now that the dataframe `df` is properly loaded.


"""

from textblob import TextBlob
import matplotlib.pyplot as plt

# Ensure 'Hashtags' column is of string type
df['Hashtags'] = df['Hashtags'].astype(str)

# Perform sentiment analysis using TextBlob (if not already done)
if 'Polarity' not in df.columns:
    df['Polarity'] = df['Text'].apply(lambda text: TextBlob(text).sentiment.polarity)

# Categorize sentiment
def categorize_sentiment(polarity):
    if polarity > 0.1:
        return 'Positive'
    elif polarity < -0.1:
        return 'Negative'
    else:
        return 'Neutral'

df['Sentiment_Category'] = df['Polarity'].apply(categorize_sentiment)

# Split hashtags into individual hashtags and explode the column
df['Hashtags'] = df['Hashtags'].str.split(',')
hashtags_df = df.explode('Hashtags')

# Remove leading/trailing whitespace from hashtags
hashtags_df['Hashtags'] = hashtags_df['Hashtags'].str.strip()

# Calculate the average sentiment for each hashtag
average_sentiment_by_hashtag = hashtags_df.groupby('Hashtags')['Polarity'].mean().sort_values(ascending=False)

# Display the results
print("\nAverage Sentiment by Hashtag:")
print(average_sentiment_by_hashtag)

# Visualize the average sentiment by hashtag
plt.figure(figsize=(12, 6))
average_sentiment_by_hashtag.plot(kind='bar', color='skyblue')
plt.title('Average Sentiment Score by Hashtag')
plt.xlabel('Hashtag')
plt.ylabel('Average Polarity Score')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Visualize overall sentiment distribution
sentiment_counts = df['Sentiment_Category'].value_counts()
plt.figure(figsize=(8, 6))
sentiment_counts.plot(kind='bar', color=['skyblue', 'salmon', 'lightgreen'])
plt.title('Distribution of Sentiment Categories')
plt.xlabel('Sentiment Category')
plt.ylabel('Frequency')
plt.show()

# Visualize the frequency of each hashtag
hashtag_counts = hashtags_df['Hashtags'].value_counts()
plt.figure(figsize=(12, 6))
hashtag_counts.head(20).plot(kind='bar', color='skyblue')  # Show top 20 hashtags
plt.title('Frequency of Top 20 Hashtags')
plt.xlabel('Hashtag')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import pandas as pd

# Load the uploaded CSV file
file_path = "/content/sentimentdataset (2).csv"
df = pd.read_csv(file_path)

# Display basic info and preview
df.info(), df.head()

from bs4 import BeautifulSoup
from textblob import TextBlob
import re

# Function to clean text
def clean_text(text):
    soup = BeautifulSoup(text, "html.parser").get_text()  # Remove HTML
    text = re.sub(r"http\S+", "", soup)  # Remove URLs
    text = re.sub(r"[^A-Za-z0-9\s#@]", "", text)  # Remove special chars (keep hashtags/mentions)
    text = text.lower().strip()  # Normalize case and strip whitespace
    return text

# Apply cleaning
df["Cleaned_Text"] = df["Text"].apply(clean_text)

# Function to analyze sentiment using TextBlob
def analyze_sentiment(text):
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity
    if polarity > 0.1:
        sentiment = "Positive"
    elif polarity < -0.1:
        sentiment = "Negative"
    else:
        sentiment = "Neutral"
    return pd.Series([polarity, sentiment])

# Apply sentiment analysis
df[["Polarity", "Calculated_Sentiment"]] = df["Cleaned_Text"].apply(analyze_sentiment)

# Preview updated data
df[["Text", "Cleaned_Text", "Sentiment", "Polarity", "Calculated_Sentiment"]].head()

from collections import Counter
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

# Tokenization helper (excluding stopwords, hashtags kept)
def extract_keywords(text):
    words = text.split()
    return [word for word in words if word not in ENGLISH_STOP_WORDS and not word.startswith("@")]

# Extract from Cleaned_Text
all_keywords = df["Cleaned_Text"].apply(extract_keywords).sum()
keyword_counts = Counter(all_keywords)

# Extract from Hashtags column
def extract_hashtags(text):
    return [tag.lower() for tag in text.split() if tag.startswith("#")]

all_hashtags = df["Hashtags"].apply(extract_hashtags).sum()
hashtag_counts = Counter(all_hashtags)

# Get top 15 from each
top_keywords = keyword_counts.most_common(15)
top_hashtags = hashtag_counts.most_common(15)

top_keywords, top_hashtags

# Select relevant columns for Power BI dashboard
export_df = df[[
    "Text", "Cleaned_Text", "Sentiment", "Polarity", "Calculated_Sentiment",
    "Hashtags", "Timestamp", "Platform", "Country", "Retweets", "Likes"
]]

# Save to CSV
export_path = "/content/sentimentdataset (2).csv"
export_df.to_csv(export_path, index=False)

export_path

# Preprocess Text Column

text_column_name = 'Text'
if text_column_name not in df.columns:
    raise KeyError(f"Column '{text_column_name}' not found in the DataFrame. Please check your dataset or update 'text_column_name'.")

df[text_column_name] = df[text_column_name].astype(str)
df['text_clean'] = df[text_column_name].str.lower().str.replace(r'[^\w\s]', '', regex=True)

# Sentiment Analysis using TextBlob
def analyze_sentiment(text):
    return TextBlob(text).sentiment.polarity

df['sentiment_score'] = df['text_clean'].apply(analyze_sentiment)
df['sentiment'] = df['sentiment_score'].apply(lambda x: 'Positive' if x > 0 else ('Negative' if x < 0 else 'Neutral'))

# Extract Trending Keywords
all_words = ' '.join(df['text_clean'])
word_freq = Counter(all_words.split())
common_words_df = pd.DataFrame(word_freq.most_common(20), columns=['Word', 'Frequency'])

# =9 Import Required Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
from wordcloud import WordCloud
from collections import Counter
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Run this cell first to define 'pd'

# Visualizations
# Sentiment Distribution Plot
plt.figure(figsize=(8, 4))
sns.countplot(data=df, x='sentiment', palette='Set2')
plt.title('Sentiment Distribution of Trending Topics')
plt.show()

# Word Cloud of Trending Topics
wordcloud = WordCloud(width=800, height=400, background_color='black').generate(all_words)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Trending Keywords WordCloud')
plt.show()

#  Export for Dashboarding
df[[text_column_name, 'sentiment_score', 'sentiment']].to_csv('sentiment_analysis_output.csv', index=False)
common_words_df.to_csv('trending_keywords.csv', index=False)